{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aef348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-table:\n",
      "[[[ 0.39308177  0.4782969   0.41502086  0.23360863]\n",
      "  [ 0.         -0.99323415  0.3766807   0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.40867507  0.531441    0.39204919 -0.53865084]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.         -1.         -1.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.45657814  0.4433358   0.47478688  0.59049   ]\n",
      "  [-0.40621767  0.6561      0.49717982 -0.42215985]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.         -0.468559   -0.74581342  0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.52978326  0.          0.0474831   0.        ]\n",
      "  [ 0.53320045  0.68266147  0.38708063  0.729     ]\n",
      "  [-0.26634638  0.81        0.59183934 -0.25701489]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.         -0.19        0.        ]]\n",
      "\n",
      " [[ 0.08997157  0.          0.          0.        ]\n",
      "  [ 0.15215929  0.07252407  0.00423945  0.80966682]\n",
      "  [ 0.65561564  0.77854878  0.71398684  0.9       ]\n",
      "  [-0.09748159  0.86476557  0.74534867  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Grid World Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=(5, 5), start=(0, 0), goal=(4, 4), obstacles=[]):\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.state = start\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self.state\n",
    "        if action == 'up':\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 'down':\n",
    "            row = min(row + 1, self.size[0] - 1)\n",
    "        elif action == 'left':\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 'right':\n",
    "            col = min(col + 1, self.size[1] - 1)\n",
    "\n",
    "        if (row, col) in self.obstacles:\n",
    "            return self.state, -1, False  # Hit an obstacle\n",
    "        elif (row, col) == self.goal:\n",
    "            return (row, col), 1, True  # Reached the goal\n",
    "\n",
    "        self.state = (row, col)\n",
    "        return self.state, 0, False  # Normal move with no reward\n",
    "\n",
    "# Q-Learning algorithm\n",
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    q_table = np.zeros(env.size + (len(env.actions),))\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(range(len(env.actions)))  # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])  # Exploit learned values\n",
    "\n",
    "            next_state, reward, done = env.step(env.actions[action])\n",
    "            next_action = np.argmax(q_table[next_state])  # Greedy action for next state\n",
    "\n",
    "            # Update Q-value\n",
    "            q_table[state][action] += alpha * (reward + gamma * q_table[next_state][next_action] - q_table[state][action])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "# Initialize the Grid World\n",
    "grid_world = GridWorld(obstacles=[(1, 1), (2, 2), (3, 3)])\n",
    "\n",
    "# Train the agent\n",
    "q_table = q_learning(grid_world)\n",
    "\n",
    "# Display the final Q-table\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc73fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da161b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
